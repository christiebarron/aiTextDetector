{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b237d23",
   "metadata": {},
   "source": [
    "# Supplemental Code\n",
    "\n",
    "## Preliminary calculation and comparison of features between AI-generated and human-written text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6c6a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc5dd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract lexical features\n",
    "def extract_lexical_features(text):\n",
    "    # ... your extract_lexical_features function implementation ...\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    total_word_count = len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    word_counts = Counter(words)\n",
    "    TTR = len(word_counts) / len(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    unique_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "    word_freq = word_counts\n",
    "    bigram_freq = Counter(ngrams(words, 2))\n",
    "    trigram_freq = Counter(ngrams(words, 3))\n",
    "    rare_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "\n",
    "    return {\n",
    "        'total_word_count': total_word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'TTR': TTR,\n",
    "        'stop_word_count': stop_word_count,\n",
    "        'unique_word_count': unique_word_count,\n",
    "        'word_freq': word_freq,\n",
    "        'bigram_freq': bigram_freq,\n",
    "        'trigram_freq': trigram_freq,\n",
    "        'rare_word_count': rare_word_count\n",
    "    }\n",
    "\n",
    "#load data from excel file and save as list\n",
    "merged_df = pd.read_excel('../cleanData/processedAsap.xlsx')\n",
    "all_essays = merged_df['essay'].tolist()\n",
    "\n",
    "#extract ai-specific dataset\n",
    "ai_generated_df = merged_df.loc[(merged_df[\"ai_generated\"] == 1), :]\n",
    "ai_generated_essays = ai_generated_df['essay'].tolist()\n",
    "\n",
    "#extract human-specific dataset\n",
    "human_written_df = merged_df.loc[(merged_df[\"ai_generated\"] == 0), :]\n",
    "human_written_essays = human_written_df['essay'].tolist()\n",
    "\n",
    "# Extract lexical features from AI-generated and human-written essays\n",
    "all_lexical_features = [extract_lexical_features(essay) for essay in all_essays]\n",
    "ai_generated_features = [extract_lexical_features(essay) for essay in ai_generated_essays]\n",
    "human_written_features = [extract_lexical_features(essay) for essay in human_written_essays]\n",
    "\n",
    "\n",
    "# Calculate average values of some lexical features for both AI-generated and human-written essays\n",
    "def average_feature_value(features, feature_key):\n",
    "    return sum(feature[feature_key] for feature in features) / len(features)\n",
    "\n",
    "ai_avg_word_length = average_feature_value(ai_generated_features, 'avg_word_length')\n",
    "ai_avg_TTR = average_feature_value(ai_generated_features, 'TTR')\n",
    "ai_avg_stop_word_count = average_feature_value(ai_generated_features, 'stop_word_count')\n",
    "\n",
    "ai_avg_sentence_length = average_feature_value(ai_generated_features, 'avg_sentence_length')\n",
    "human_avg_sentence_length = average_feature_value(human_written_features, 'avg_sentence_length')\n",
    "human_avg_word_length = average_feature_value(human_written_features, 'avg_word_length')\n",
    "human_avg_TTR = average_feature_value(human_written_features, 'TTR')\n",
    "human_avg_stop_word_count = average_feature_value(human_written_features, 'stop_word_count')\n",
    "\n",
    "# Calculate average values of the total word count for both AI-generated and human-written essays\n",
    "ai_avg_total_word_count = average_feature_value(ai_generated_features, 'total_word_count')\n",
    "human_avg_total_word_count = average_feature_value(human_written_features, 'total_word_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c23d287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>TTR</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>word_freq</th>\n",
       "      <th>bigram_freq</th>\n",
       "      <th>trigram_freq</th>\n",
       "      <th>rare_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>21.125000</td>\n",
       "      <td>0.468912</td>\n",
       "      <td>176</td>\n",
       "      <td>120</td>\n",
       "      <td>{'Dear': 1, 'local': 2, 'newspaper': 1, ',': 1...</td>\n",
       "      <td>{('Dear', 'local'): 1, ('local', 'newspaper'):...</td>\n",
       "      <td>{('Dear', 'local', 'newspaper'): 1, ('local', ...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>464</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>20.950000</td>\n",
       "      <td>0.450431</td>\n",
       "      <td>195</td>\n",
       "      <td>127</td>\n",
       "      <td>{'Dear': 1, '@': 10, 'CAPS1': 1, 'CAPS2': 1, '...</td>\n",
       "      <td>{('Dear', '@'): 1, ('@', 'CAPS1'): 1, ('CAPS1'...</td>\n",
       "      <td>{('Dear', '@', 'CAPS1'): 1, ('@', 'CAPS1', '@'...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>313</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>19.928571</td>\n",
       "      <td>0.514377</td>\n",
       "      <td>143</td>\n",
       "      <td>111</td>\n",
       "      <td>{'Dear': 1, ',': 9, '@': 7, 'CAPS1': 1, 'CAPS2...</td>\n",
       "      <td>{('Dear', ','): 1, (',', '@'): 2, ('@', 'CAPS1...</td>\n",
       "      <td>{('Dear', ',', '@'): 1, (',', '@', 'CAPS1'): 1...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>611</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>19.407407</td>\n",
       "      <td>0.436989</td>\n",
       "      <td>223</td>\n",
       "      <td>182</td>\n",
       "      <td>{'Dear': 1, 'Local': 1, 'Newspaper': 3, ',': 1...</td>\n",
       "      <td>{('Dear', 'Local'): 1, ('Local', 'Newspaper'):...</td>\n",
       "      <td>{('Dear', 'Local', 'Newspaper'): 1, ('Local', ...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.408124</td>\n",
       "      <td>241</td>\n",
       "      <td>125</td>\n",
       "      <td>{'Dear': 1, '@': 4, 'LOCATION1': 1, ',': 13, '...</td>\n",
       "      <td>{('Dear', '@'): 1, ('@', 'LOCATION1'): 1, ('LO...</td>\n",
       "      <td>{('Dear', '@', 'LOCATION1'): 1, ('@', 'LOCATIO...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24412</th>\n",
       "      <td>274</td>\n",
       "      <td>4.664234</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.492701</td>\n",
       "      <td>119</td>\n",
       "      <td>90</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 16, 'As': 1, 'a'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24413</th>\n",
       "      <td>247</td>\n",
       "      <td>4.619433</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.603239</td>\n",
       "      <td>101</td>\n",
       "      <td>114</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 14, 'As': 1, 'a'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24414</th>\n",
       "      <td>216</td>\n",
       "      <td>4.717593</td>\n",
       "      <td>15.583333</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>77</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 12, 'I': 2, 'am'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24415</th>\n",
       "      <td>272</td>\n",
       "      <td>4.977941</td>\n",
       "      <td>19.916667</td>\n",
       "      <td>0.566176</td>\n",
       "      <td>106</td>\n",
       "      <td>110</td>\n",
       "      <td>{'To': 2, 'the': 9, 'Editor': 1, ':': 1, 'As':...</td>\n",
       "      <td>{('To', 'the'): 1, ('the', 'Editor'): 1, ('Edi...</td>\n",
       "      <td>{('To', 'the', 'Editor'): 1, ('the', 'Editor',...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24416</th>\n",
       "      <td>281</td>\n",
       "      <td>4.569395</td>\n",
       "      <td>15.437500</td>\n",
       "      <td>0.523132</td>\n",
       "      <td>107</td>\n",
       "      <td>104</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 19, 'I': 4, 'am'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24417 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_word_count  avg_word_length  avg_sentence_length       TTR  \\\n",
       "0                   386         3.984456            21.125000  0.468912   \n",
       "1                   464         4.030172            20.950000  0.450431   \n",
       "2                   313         4.035144            19.928571  0.514377   \n",
       "3                   611         4.328969            19.407407  0.436989   \n",
       "4                   517         4.071567            15.500000  0.408124   \n",
       "...                 ...              ...                  ...       ...   \n",
       "24412               274         4.664234            20.166667  0.492701   \n",
       "24413               247         4.619433            18.000000  0.603239   \n",
       "24414               216         4.717593            15.583333  0.601852   \n",
       "24415               272         4.977941            19.916667  0.566176   \n",
       "24416               281         4.569395            15.437500  0.523132   \n",
       "\n",
       "       stop_word_count  unique_word_count  \\\n",
       "0                  176                120   \n",
       "1                  195                127   \n",
       "2                  143                111   \n",
       "3                  223                182   \n",
       "4                  241                125   \n",
       "...                ...                ...   \n",
       "24412              119                 90   \n",
       "24413              101                114   \n",
       "24414               77                100   \n",
       "24415              106                110   \n",
       "24416              107                104   \n",
       "\n",
       "                                               word_freq  \\\n",
       "0      {'Dear': 1, 'local': 2, 'newspaper': 1, ',': 1...   \n",
       "1      {'Dear': 1, '@': 10, 'CAPS1': 1, 'CAPS2': 1, '...   \n",
       "2      {'Dear': 1, ',': 9, '@': 7, 'CAPS1': 1, 'CAPS2...   \n",
       "3      {'Dear': 1, 'Local': 1, 'Newspaper': 3, ',': 1...   \n",
       "4      {'Dear': 1, '@': 4, 'LOCATION1': 1, ',': 13, '...   \n",
       "...                                                  ...   \n",
       "24412  {'Dear': 1, 'Editor': 1, ',': 16, 'As': 1, 'a'...   \n",
       "24413  {'Dear': 1, 'Editor': 1, ',': 14, 'As': 1, 'a'...   \n",
       "24414  {'Dear': 1, 'Editor': 1, ',': 12, 'I': 2, 'am'...   \n",
       "24415  {'To': 2, 'the': 9, 'Editor': 1, ':': 1, 'As':...   \n",
       "24416  {'Dear': 1, 'Editor': 1, ',': 19, 'I': 4, 'am'...   \n",
       "\n",
       "                                             bigram_freq  \\\n",
       "0      {('Dear', 'local'): 1, ('local', 'newspaper'):...   \n",
       "1      {('Dear', '@'): 1, ('@', 'CAPS1'): 1, ('CAPS1'...   \n",
       "2      {('Dear', ','): 1, (',', '@'): 2, ('@', 'CAPS1...   \n",
       "3      {('Dear', 'Local'): 1, ('Local', 'Newspaper'):...   \n",
       "4      {('Dear', '@'): 1, ('@', 'LOCATION1'): 1, ('LO...   \n",
       "...                                                  ...   \n",
       "24412  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "24413  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "24414  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "24415  {('To', 'the'): 1, ('the', 'Editor'): 1, ('Edi...   \n",
       "24416  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "\n",
       "                                            trigram_freq  rare_word_count  \n",
       "0      {('Dear', 'local', 'newspaper'): 1, ('local', ...              120  \n",
       "1      {('Dear', '@', 'CAPS1'): 1, ('@', 'CAPS1', '@'...              127  \n",
       "2      {('Dear', ',', '@'): 1, (',', '@', 'CAPS1'): 1...              111  \n",
       "3      {('Dear', 'Local', 'Newspaper'): 1, ('Local', ...              182  \n",
       "4      {('Dear', '@', 'LOCATION1'): 1, ('@', 'LOCATIO...              125  \n",
       "...                                                  ...              ...  \n",
       "24412  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...               90  \n",
       "24413  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              114  \n",
       "24414  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              100  \n",
       "24415  {('To', 'the', 'Editor'): 1, ('the', 'Editor',...              110  \n",
       "24416  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              104  \n",
       "\n",
       "[24417 rows x 10 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(merged_df, pd.DataFrame(all_lexical_features)).to_excel(\"../cleanData/Feature Extraction/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b99076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract syntactic features\n",
    "def extract_syntactic_features(text):\n",
    "    # ... your extract_syntactic_features function implementation ...\n",
    "    doc = nlp_spacy(text)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    avg_sentence_length = np.mean(sentence_lengths)\n",
    "\n",
    "    # Calculate parse tree depth\n",
    "    def calc_tree_depth(sent):\n",
    "        root = [token for token in sent if token.head == token][0]\n",
    "        return max([len(list(token.ancestors)) for token in sent])\n",
    "\n",
    "    tree_depths = [calc_tree_depth(sent) for sent in doc.sents]\n",
    "    avg_parse_tree_depth = np.mean(tree_depths)\n",
    "    parse_tree_depth_variation = np.std(tree_depths)\n",
    "\n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_parse_tree_depth': avg_parse_tree_depth,\n",
    "        'parse_tree_depth_variation': parse_tree_depth_variation,\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract syntactic features from AI-generated and human-written essays\n",
    "all_syntactic_features = [extract_syntactic_features(essay) for essay in all_essays]\n",
    "ai_generated_syntactic_features = [extract_syntactic_features(essay) for essay in ai_generated_essays]\n",
    "human_written_syntactic_features = [extract_syntactic_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Compare the syntactic features\n",
    "ai_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in ai_generated_syntactic_features])\n",
    "human_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in ai_generated_syntactic_features])\n",
    "human_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in ai_generated_syntactic_features])\n",
    "human_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in human_written_syntactic_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873a75b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>avg_parse_tree_depth</th>\n",
       "      <th>parse_tree_depth_variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.687500</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2.410913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.826087</td>\n",
       "      <td>5.565217</td>\n",
       "      <td>2.990217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.857143</td>\n",
       "      <td>5.214286</td>\n",
       "      <td>1.779991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.333333</td>\n",
       "      <td>4.925926</td>\n",
       "      <td>2.017074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.266667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>1.593389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24412</th>\n",
       "      <td>24.250000</td>\n",
       "      <td>6.583333</td>\n",
       "      <td>1.255543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24413</th>\n",
       "      <td>21.166667</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>1.605113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24414</th>\n",
       "      <td>18.916667</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.101587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24415</th>\n",
       "      <td>23.750000</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>1.656217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24416</th>\n",
       "      <td>18.062500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.061553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24417 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_sentence_length  avg_parse_tree_depth  parse_tree_depth_variation\n",
       "0                24.687500              5.250000                    2.410913\n",
       "1                19.826087              5.565217                    2.990217\n",
       "2                21.857143              5.214286                    1.779991\n",
       "3                21.333333              4.925926                    2.017074\n",
       "4                17.266667              4.833333                    1.593389\n",
       "...                    ...                   ...                         ...\n",
       "24412            24.250000              6.583333                    1.255543\n",
       "24413            21.166667              4.916667                    1.605113\n",
       "24414            18.916667              5.500000                    2.101587\n",
       "24415            23.750000              5.416667                    1.656217\n",
       "24416            18.062500              5.000000                    2.061553\n",
       "\n",
       "[24417 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_syntactic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4793bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lexical and syntactic features\n",
    "def combined_features(text):\n",
    "    lexical = extract_lexical_features(text)\n",
    "    syntactic = extract_syntactic_features(text)\n",
    "    return {**lexical, **syntactic}\n",
    "\n",
    "# Extract combined features for AI-generated and human-written essays\n",
    "all_combined_features = [combined_features(essay) for essay in all_essays]\n",
    "ai_generated_combined_features = [combined_features(essay) for essay in ai_generated_essays]\n",
    "human_written_combined_features = [combined_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Create a DataFrame for AI-generated essays\n",
    "ai_generated_df = pd.DataFrame(ai_generated_combined_features)\n",
    "ai_generated_df['type'] = 'AI-generated'\n",
    "\n",
    "# Create a DataFrame for human-written essays\n",
    "human_written_df = pd.DataFrame(human_written_combined_features)\n",
    "human_written_df['type'] = 'Human-written'\n",
    "\n",
    "# Calculate the average values of features for both AI-generated and human-written essays\n",
    "def average_feature_value(features, feature_key):\n",
    "    return sum(feature[feature_key] for feature in features) / len(features)\n",
    "\n",
    "# Define a list of feature keys to extract from the combined features\n",
    "feature_keys = [\n",
    "    'total_word_count',\n",
    "    'avg_word_length',\n",
    "    'avg_sentence_length',\n",
    "    'TTR',\n",
    "    'stop_word_count',\n",
    "    'unique_word_count',\n",
    "    'rare_word_count',\n",
    "    'avg_parse_tree_depth',\n",
    "    'parse_tree_depth_variation'\n",
    "]\n",
    "\n",
    "# Calculate the average values for each feature\n",
    "ai_generated_avgs = [average_feature_value(ai_generated_combined_features, key) for key in feature_keys]\n",
    "human_written_avgs = [average_feature_value(human_written_combined_features, key) for key in feature_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5864d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stylistic Features\n",
    "def extract_stylistic_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    pos_tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    \n",
    "    num_adjectives = sum(sum(1 for word, pos in sentence if pos.startswith('JJ')) for sentence in pos_tagged_sentences)\n",
    "    num_adverbs = sum(sum(1 for word, pos in sentence if pos.startswith('RB')) for sentence in pos_tagged_sentences)\n",
    "    num_verbs = sum(sum(1 for word, pos in sentence if pos.startswith('VB')) for sentence in pos_tagged_sentences)\n",
    "    num_nouns = sum(sum(1 for word, pos in sentence if pos.startswith('NN')) for sentence in pos_tagged_sentences)\n",
    "\n",
    "    avg_adjectives_per_sentence = num_adjectives / num_sentences\n",
    "    avg_adverbs_per_sentence = num_adverbs / num_sentences\n",
    "    avg_verbs_per_sentence = num_verbs / num_sentences\n",
    "    avg_nouns_per_sentence = num_nouns / num_sentences\n",
    "    \n",
    "    return {\n",
    "        'avg_adjectives_per_sentence': avg_adjectives_per_sentence,\n",
    "        'avg_adverbs_per_sentence': avg_adverbs_per_sentence,\n",
    "        'avg_verbs_per_sentence': avg_verbs_per_sentence,\n",
    "        'avg_nouns_per_sentence': avg_nouns_per_sentence,\n",
    "    }\n",
    "\n",
    "# Extract stylistic features from AI-generated and human-written essays\n",
    "all_stylistic_features = [extract_stylistic_features(essay) for essay in all_essays]\n",
    "ai_generated_stylistic_features = [extract_stylistic_features(essay) for essay in ai_generated_essays]\n",
    "human_written_stylistic_features = [extract_stylistic_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Calculate average values of stylistic features for both AI-generated and human-written essays\n",
    "ai_avg_adjectives_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "ai_avg_adverbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "ai_avg_verbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_verbs_per_sentence')\n",
    "ai_avg_nouns_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "human_avg_adjectives_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "human_avg_adverbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "human_avg_verbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_verbs_per_sentence')\n",
    "human_avg_nouns_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
    "    return punctuation_count\n",
    "\n",
    "def average_value(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "all_avg_punctuation = average_value([count_punctuation(essay) for essay in all_essays])\n",
    "ai_avg_punctuation = average_value([count_punctuation(essay) for essay in ai_generated_essays])\n",
    "human_avg_punctuation = average_value([count_punctuation(essay) for essay in human_written_essays])\n",
    "\n",
    "\n",
    "# ai_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in ai_generated_essays])\n",
    "# human_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in human_written_essays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syntactic_features, all_stylistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08a2bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the extracted features with the original data\n",
    "features_df = pd.concat([merged_df, pd.DataFrame(all_lexical_features), pd.DataFrame(all_syntactic_features), pd.DataFrame(all_stylistic_features)], axis = 1)\n",
    "\n",
    "#save as excel document\n",
    "features_df.to_excel(\"../cleanData/featuresAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ce4d1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>ai_llm</th>\n",
       "      <th>ai_generated</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>lemmatized_word_tokens</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>avg_parse_tree_depth</th>\n",
       "      <th>parse_tree_depth_variation</th>\n",
       "      <th>avg_adjectives_per_sentence</th>\n",
       "      <th>avg_adverbs_per_sentence</th>\n",
       "      <th>avg_verbs_per_sentence</th>\n",
       "      <th>avg_nouns_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>['dear', 'local', 'newspaper', 'think', 'effec...</td>\n",
       "      <td>['dear local newspaper, i think effects comput...</td>\n",
       "      <td>['dear', 'local', 'newspaper', 'think', 'effec...</td>\n",
       "      <td>24.687500</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2.410913</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>['dear', 'caps1', 'caps2', 'believe', 'using',...</td>\n",
       "      <td>['dear @caps1 @caps2, i believe that using com...</td>\n",
       "      <td>['dear', 'caps1', 'caps2', 'believe', 'using',...</td>\n",
       "      <td>19.826087</td>\n",
       "      <td>5.565217</td>\n",
       "      <td>2.990217</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>['dear', 'caps1', 'caps2', 'caps3', 'people', ...</td>\n",
       "      <td>['dear, @caps1 @caps2 @caps3 more and more peo...</td>\n",
       "      <td>['dear', 'caps1', 'caps2', 'caps3', 'people', ...</td>\n",
       "      <td>21.857143</td>\n",
       "      <td>5.214286</td>\n",
       "      <td>1.779991</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>3.785714</td>\n",
       "      <td>6.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>['dear', 'local', 'newspaper', 'caps1', 'found...</td>\n",
       "      <td>['dear local newspaper, @caps1 i have found th...</td>\n",
       "      <td>['dear', 'local', 'newspaper', 'caps1', 'found...</td>\n",
       "      <td>21.333333</td>\n",
       "      <td>4.925926</td>\n",
       "      <td>2.017074</td>\n",
       "      <td>1.925926</td>\n",
       "      <td>1.074074</td>\n",
       "      <td>3.629630</td>\n",
       "      <td>7.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>['dear', 'location1', 'know', 'computers', 'po...</td>\n",
       "      <td>['dear @location1, i know having computers has...</td>\n",
       "      <td>['dear', 'location1', 'know', 'computer', 'pos...</td>\n",
       "      <td>17.266667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>1.593389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>3.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24412</th>\n",
       "      <td>24412</td>\n",
       "      <td>213</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nDear Editor, \\n\\nAs a citizen of this comm...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>['dear', 'editor', 'citizen', 'community', 'fe...</td>\n",
       "      <td>[\"\\n\\ndear editor, \\n\\nas a citizen of this co...</td>\n",
       "      <td>['dear', 'editor', 'citizen', 'community', 'fe...</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>6.583333</td>\n",
       "      <td>1.255543</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>5.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24413</th>\n",
       "      <td>24413</td>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\nDear Editor,\\n\\nAs a concerned citizen a...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>['dear', 'editor', 'concerned', 'citizen', 'lo...</td>\n",
       "      <td>['\\n\\n\\ndear editor,\\n\\nas a concerned citizen...</td>\n",
       "      <td>['dear', 'editor', 'concerned', 'citizen', 'lo...</td>\n",
       "      <td>21.166667</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>1.605113</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24414</th>\n",
       "      <td>24414</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nDear Editor,\\n\\nI am writing to share my o...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>['dear', 'editor', 'writing', 'share', 'opinio...</td>\n",
       "      <td>['\\n\\ndear editor,\\n\\ni am writing to share my...</td>\n",
       "      <td>['dear', 'editor', 'writing', 'share', 'opinio...</td>\n",
       "      <td>18.916667</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.101587</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24415</th>\n",
       "      <td>24415</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nTo the Editor: \\n\\nAs our world has become...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>['editor', 'world', 'become', 'increasingly', ...</td>\n",
       "      <td>['\\n\\nto the editor: \\n\\nas our world has beco...</td>\n",
       "      <td>['editor', 'world', 'become', 'increasingly', ...</td>\n",
       "      <td>23.750000</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>1.656217</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24416</th>\n",
       "      <td>24416</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nDear Editor, \\n\\nI am writing to present m...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>['dear', 'editor', 'writing', 'present', 'thou...</td>\n",
       "      <td>['\\n\\ndear editor, \\n\\ni am writing to present...</td>\n",
       "      <td>['dear', 'editor', 'writing', 'present', 'thou...</td>\n",
       "      <td>18.062500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.061553</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24417 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  essay_id  essay_set  \\\n",
       "0               0         1          1   \n",
       "1               1         2          1   \n",
       "2               2         3          1   \n",
       "3               3         4          1   \n",
       "4               4         5          1   \n",
       "...           ...       ...        ...   \n",
       "24412       24412       213          1   \n",
       "24413       24413       214          1   \n",
       "24414       24414       215          1   \n",
       "24415       24415       216          1   \n",
       "24416       24416       217          1   \n",
       "\n",
       "                                                   essay            ai_llm  \\\n",
       "0      Dear local newspaper, I think effects computer...   human-generated   \n",
       "1      Dear @CAPS1 @CAPS2, I believe that using compu...   human-generated   \n",
       "2      Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   human-generated   \n",
       "3      Dear Local Newspaper, @CAPS1 I have found that...   human-generated   \n",
       "4      Dear @LOCATION1, I know having computers has a...   human-generated   \n",
       "...                                                  ...               ...   \n",
       "24412  \\n\\nDear Editor, \\n\\nAs a citizen of this comm...  text-davinci-003   \n",
       "24413  \\n\\n\\nDear Editor,\\n\\nAs a concerned citizen a...  text-davinci-003   \n",
       "24414  \\n\\nDear Editor,\\n\\nI am writing to share my o...  text-davinci-003   \n",
       "24415  \\n\\nTo the Editor: \\n\\nAs our world has become...  text-davinci-003   \n",
       "24416  \\n\\nDear Editor, \\n\\nI am writing to present m...  text-davinci-003   \n",
       "\n",
       "       ai_generated                                        word_tokens  \\\n",
       "0                 0  ['dear', 'local', 'newspaper', 'think', 'effec...   \n",
       "1                 0  ['dear', 'caps1', 'caps2', 'believe', 'using',...   \n",
       "2                 0  ['dear', 'caps1', 'caps2', 'caps3', 'people', ...   \n",
       "3                 0  ['dear', 'local', 'newspaper', 'caps1', 'found...   \n",
       "4                 0  ['dear', 'location1', 'know', 'computers', 'po...   \n",
       "...             ...                                                ...   \n",
       "24412             1  ['dear', 'editor', 'citizen', 'community', 'fe...   \n",
       "24413             1  ['dear', 'editor', 'concerned', 'citizen', 'lo...   \n",
       "24414             1  ['dear', 'editor', 'writing', 'share', 'opinio...   \n",
       "24415             1  ['editor', 'world', 'become', 'increasingly', ...   \n",
       "24416             1  ['dear', 'editor', 'writing', 'present', 'thou...   \n",
       "\n",
       "                                         sentence_tokens  \\\n",
       "0      ['dear local newspaper, i think effects comput...   \n",
       "1      ['dear @caps1 @caps2, i believe that using com...   \n",
       "2      ['dear, @caps1 @caps2 @caps3 more and more peo...   \n",
       "3      ['dear local newspaper, @caps1 i have found th...   \n",
       "4      ['dear @location1, i know having computers has...   \n",
       "...                                                  ...   \n",
       "24412  [\"\\n\\ndear editor, \\n\\nas a citizen of this co...   \n",
       "24413  ['\\n\\n\\ndear editor,\\n\\nas a concerned citizen...   \n",
       "24414  ['\\n\\ndear editor,\\n\\ni am writing to share my...   \n",
       "24415  ['\\n\\nto the editor: \\n\\nas our world has beco...   \n",
       "24416  ['\\n\\ndear editor, \\n\\ni am writing to present...   \n",
       "\n",
       "                                  lemmatized_word_tokens  avg_sentence_length  \\\n",
       "0      ['dear', 'local', 'newspaper', 'think', 'effec...            24.687500   \n",
       "1      ['dear', 'caps1', 'caps2', 'believe', 'using',...            19.826087   \n",
       "2      ['dear', 'caps1', 'caps2', 'caps3', 'people', ...            21.857143   \n",
       "3      ['dear', 'local', 'newspaper', 'caps1', 'found...            21.333333   \n",
       "4      ['dear', 'location1', 'know', 'computer', 'pos...            17.266667   \n",
       "...                                                  ...                  ...   \n",
       "24412  ['dear', 'editor', 'citizen', 'community', 'fe...            24.250000   \n",
       "24413  ['dear', 'editor', 'concerned', 'citizen', 'lo...            21.166667   \n",
       "24414  ['dear', 'editor', 'writing', 'share', 'opinio...            18.916667   \n",
       "24415  ['editor', 'world', 'become', 'increasingly', ...            23.750000   \n",
       "24416  ['dear', 'editor', 'writing', 'present', 'thou...            18.062500   \n",
       "\n",
       "       avg_parse_tree_depth  parse_tree_depth_variation  \\\n",
       "0                  5.250000                    2.410913   \n",
       "1                  5.565217                    2.990217   \n",
       "2                  5.214286                    1.779991   \n",
       "3                  4.925926                    2.017074   \n",
       "4                  4.833333                    1.593389   \n",
       "...                     ...                         ...   \n",
       "24412              6.583333                    1.255543   \n",
       "24413              4.916667                    1.605113   \n",
       "24414              5.500000                    2.101587   \n",
       "24415              5.416667                    1.656217   \n",
       "24416              5.000000                    2.061553   \n",
       "\n",
       "       avg_adjectives_per_sentence  avg_adverbs_per_sentence  \\\n",
       "0                         1.250000                  1.437500   \n",
       "1                         1.050000                  0.900000   \n",
       "2                         1.357143                  1.071429   \n",
       "3                         1.925926                  1.074074   \n",
       "4                         1.000000                  1.266667   \n",
       "...                            ...                       ...   \n",
       "24412                     1.833333                  1.166667   \n",
       "24413                     2.250000                  0.750000   \n",
       "24414                     1.500000                  0.583333   \n",
       "24415                     2.250000                  1.166667   \n",
       "24416                     1.562500                  1.125000   \n",
       "\n",
       "       avg_verbs_per_sentence  avg_nouns_per_sentence  \n",
       "0                    4.250000                5.250000  \n",
       "1                    4.250000                5.700000  \n",
       "2                    3.785714                6.142857  \n",
       "3                    3.629630                7.407407  \n",
       "4                    2.900000                3.866667  \n",
       "...                       ...                     ...  \n",
       "24412                3.500000                5.166667  \n",
       "24413                2.416667                5.833333  \n",
       "24414                3.166667                5.000000  \n",
       "24415                4.500000                5.750000  \n",
       "24416                2.687500                4.250000  \n",
       "\n",
       "[24417 rows x 16 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3dc250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', \n",
    "                'Average Word Length', \n",
    "                'Average Sentence Length', \n",
    "                'Type-Token Ratio', \n",
    "                'Stop Word Count', \n",
    "                'Average Parse Tree Depth', \n",
    "                'Parse Tree Depth Variation', \n",
    "                'Average Adjectives per Sentence', \n",
    "                'Average Adverbs per Sentence', \n",
    "                'Average Verbs per Sentence', \n",
    "                'Average Nouns per Sentence', \n",
    "                'Average Punctuation Marks'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, \n",
    "                     ai_avg_word_length, ai_avg_sentence_length, \n",
    "                     ai_avg_TTR, ai_avg_stop_word_count, \n",
    "                     ai_avg_parse_tree_depth, \n",
    "                     ai_parse_tree_depth_variation, \n",
    "                     ai_avg_adjectives_per_sentence, \n",
    "                     ai_avg_adverbs_per_sentence, \n",
    "                     ai_avg_verbs_per_sentence, \n",
    "                     ai_avg_nouns_per_sentence, \n",
    "                     ai_avg_punctuation],\n",
    "    'Human-Written': [human_avg_total_word_count, \n",
    "                      human_avg_word_length, \n",
    "                      human_avg_sentence_length, \n",
    "                      human_avg_TTR, \n",
    "                      human_avg_stop_word_count, \n",
    "                      human_avg_parse_tree_depth, \n",
    "                      human_parse_tree_depth_variation, \n",
    "                      human_avg_adjectives_per_sentence, \n",
    "                      human_avg_adverbs_per_sentence, \n",
    "                      human_avg_verbs_per_sentence, \n",
    "                      human_avg_nouns_per_sentence, \n",
    "                      human_avg_punctuation],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# # Save the updated comparison DataFrame to an Excel file\n",
    "# comparison_df.to_excel('feature_comparison.xlsx', index=False)\n",
    "import openpyxl\n",
    "# Save the comparison DataFrame to an Excel file\n",
    "file_name = 'feature_comparison.xlsx'\n",
    "comparison_df.to_excel(file_name, index=False)\n",
    "\n",
    "# Autofit the column widths using openpyxl\n",
    "workbook = openpyxl.load_workbook(file_name)\n",
    "worksheet = workbook.active\n",
    "\n",
    "for column_cells in worksheet.columns:\n",
    "    length = max(len(str(cell.value)) for cell in column_cells)\n",
    "    worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "workbook.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ce3b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>AI-Generated</th>\n",
       "      <th>Human-Written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Word Count</td>\n",
       "      <td>247.460000</td>\n",
       "      <td>384.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average Word Length</td>\n",
       "      <td>4.194879</td>\n",
       "      <td>3.994626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Average Sentence Length</td>\n",
       "      <td>21.575748</td>\n",
       "      <td>18.759387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Type-Token Ratio</td>\n",
       "      <td>0.529432</td>\n",
       "      <td>0.472312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Word Count</td>\n",
       "      <td>111.480000</td>\n",
       "      <td>167.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Avg Parse Tree Depth</td>\n",
       "      <td>5.554981</td>\n",
       "      <td>4.802128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parse Tree Depth Variation</td>\n",
       "      <td>1.851215</td>\n",
       "      <td>1.676963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Punctuation Count</td>\n",
       "      <td>24.340000</td>\n",
       "      <td>47.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Passive Sentences</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Flesch Reading Ease</td>\n",
       "      <td>64.923400</td>\n",
       "      <td>73.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SMOG Index</td>\n",
       "      <td>10.060000</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentiment Polarity</td>\n",
       "      <td>0.109202</td>\n",
       "      <td>0.187748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentiment Subjectivity</td>\n",
       "      <td>0.521389</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature  AI-Generated  Human-Written\n",
       "0             Total Word Count    247.460000     384.060000\n",
       "1          Average Word Length      4.194879       3.994626\n",
       "2      Average Sentence Length     21.575748      18.759387\n",
       "3             Type-Token Ratio      0.529432       0.472312\n",
       "4              Stop Word Count    111.480000     167.880000\n",
       "5         Avg Parse Tree Depth      5.554981       4.802128\n",
       "6   Parse Tree Depth Variation      1.851215       1.676963\n",
       "7            Punctuation Count     24.340000      47.760000\n",
       "8            Passive Sentences      1.680000       1.080000\n",
       "9          Flesch Reading Ease     64.923400      73.179600\n",
       "10                  SMOG Index     10.060000       7.900000\n",
       "11          Sentiment Polarity      0.109202       0.187748\n",
       "12      Sentiment Subjectivity      0.521389       0.465000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to count passive sentences\n",
    "def count_passive_sentences(text):\n",
    "    passive_sentences = 0\n",
    "    doc = nlp_spacy(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            passive_sentences += 1\n",
    "    return passive_sentences\n",
    "\n",
    "# Function to calculate readability scores\n",
    "from readability import Readability\n",
    "from readability.exceptions import ReadabilityException\n",
    "\n",
    "\n",
    "import textstat\n",
    "\n",
    "# Function to calculate readability scores\n",
    "def readability_scores(text):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.text_standard(text, float_output=True)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level, smog_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate sentiment analysis scores\n",
    "def sentiment_analysis_scores(text):\n",
    "    sentiment = TextBlob(text)\n",
    "    return sentiment.polarity, sentiment.subjectivity\n",
    "\n",
    "# Calculate new features for AI-generated and human-written essays\n",
    "ai_generated_passive_sentences = [count_passive_sentences(essay) for essay in ai_generated_essays]\n",
    "human_written_passive_sentences = [count_passive_sentences(essay) for essay in human_written_essays]\n",
    "\n",
    "ai_generated_readability_scores = [readability_scores(essay) for essay in ai_generated_essays]\n",
    "human_written_readability_scores = [readability_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "ai_generated_sentiment_scores = [sentiment_analysis_scores(essay) for essay in ai_generated_essays]\n",
    "human_written_sentiment_scores = [sentiment_analysis_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "# Calculate average values for the new features\n",
    "ai_avg_passive_sentences = np.mean(ai_generated_passive_sentences)\n",
    "human_avg_passive_sentences = np.mean(human_written_passive_sentences)\n",
    "\n",
    "ai_avg_flesch_reading_ease = np.mean([score[0] for score in ai_generated_readability_scores])\n",
    "human_avg_flesch_reading_ease = np.mean([score[0] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_smog_index = np.mean([score[1] for score in ai_generated_readability_scores])\n",
    "human_avg_smog_index = np.mean([score[1] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_polarity = np.mean([score[0] for score in ai_generated_sentiment_scores])\n",
    "human_avg_polarity = np.mean([score[0] for score in human_written_sentiment_scores])\n",
    "\n",
    "ai_avg_subjectivity = np.mean([score[1] for score in ai_generated_sentiment_scores])\n",
    "human_avg_subjectivity = np.mean([score[1] for score in human_written_sentiment_scores])\n",
    "\n",
    "# Update comparison_data with the new features\n",
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', 'Average Word Length', 'Average Sentence Length', 'Type-Token Ratio', 'Stop Word Count', 'Avg Parse Tree Depth', 'Parse Tree Depth Variation', 'Punctuation Count', 'Passive Sentences', 'Flesch Reading Ease', 'SMOG Index', 'Sentiment Polarity', 'Sentiment Subjectivity'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, ai_avg_word_length, ai_avg_sentence_length, ai_avg_TTR, ai_avg_stop_word_count, ai_avg_parse_tree_depth, ai_parse_tree_depth_variation, ai_avg_punctuation, ai_avg_passive_sentences, ai_avg_flesch_reading_ease, ai_avg_smog_index, ai_avg_polarity, ai_avg_subjectivity],\n",
    "    'Human-Written': [human_avg_total_word_count, human_avg_word_length, human_avg_sentence_length, human_avg_TTR, human_avg_stop_word_count, human_avg_parse_tree_depth, human_parse_tree_depth_variation, human_avg_punctuation, human_avg_passive_sentences, human_avg_flesch_reading_ease, human_avg_smog_index, human_avg_polarity, human_avg_subjectivity],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725723a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
