{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline Jupyter Notebook for the aiTextDetect Project: Step 2 \n",
    "\n",
    "## Preprocessing the Text Data\n",
    "\n",
    "This script conducts a variety of text pre-processing strategies on the merged data from step 1. Text pre-processing conducted includes: cleaning text, tokenizing text, removing special characters, case conversion, correcting spellings, removing stopwords and other unnecessary terms, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cbarron/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#install dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer() #specifying wn as the word net lemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'essay_id', 'essay_set', 'essay', 'ai_llm',\n",
       "       'ai_generated'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the data\n",
    "asap_df = pd.read_excel(\"../cleanData/3aMergedAsap.xlsx\")\n",
    "\n",
    "#save a test row\n",
    "test_row = asap_df.loc[2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the raw text: remove punctuation, tokenize, stopword removal\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_text(text, tokenize_sentence):\n",
    "    sentences = nltk.sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    tokens = nltk.tokenize.word_tokenize(text) #tokenize\n",
    "    text = [word for word in tokens if word not in stopwords] #remove stopwords\n",
    "\n",
    "#determining whether to return the tokenized words (when != y) or return tokenized sentences (when == y)\n",
    "    if (tokenize_sentence == \"y\"):\n",
    "        return(sentences)\n",
    "    else:\n",
    "        return(text)\n",
    "\n",
    "\n",
    "asap_df['word_tokens'] = asap_df[\"essay\"].apply(lambda x: clean_text(x.lower(), tokenize_sentence= \"n\")) #run the new function through every row of text\n",
    "\n",
    "asap_df[\"sentence_tokens\"] = asap_df[\"essay\"].apply(lambda x: clean_text(x.lower(), tokenize_sentence= \"y\"))\n",
    "\n",
    "#def stem_text(cleaned_text_to_stem):\n",
    "#    text = [ps.stem(word) for word in cleaned_text_to_stem]\n",
    "#    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [dear, local, newspaper, think, effects, compu...\n",
       "1        [dear, caps1, caps2, believe, using, computers...\n",
       "2        [dear, caps1, caps2, caps3, people, use, compu...\n",
       "3        [dear, local, newspaper, caps1, found, many, e...\n",
       "4        [dear, location1, know, computers, positive, e...\n",
       "                               ...                        \n",
       "24412    [dear, editor, citizen, community, feel, impor...\n",
       "24413    [dear, editor, concerned, citizen, longtime, r...\n",
       "24414    [dear, editor, writing, share, opinion, effect...\n",
       "24415    [editor, world, become, increasingly, reliant,...\n",
       "24416    [dear, editor, writing, present, thoughts, eff...\n",
       "Name: word_tokens, Length: 24417, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that worked\n",
    "asap_df[\"word_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to lemmatize the text\n",
    "def lemmatize_text(word_tokens):\n",
    "    lem_text = [wn.lemmatize(word) for word in word_tokens]\n",
    "    return(lem_text)\n",
    "\n",
    "#lemmatize the text and save in a new column\n",
    "asap_df[\"lemmatized_word_tokens\"] = asap_df[\"word_tokens\"].apply(lambda x: lemmatize_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [dear, local, newspaper, think, effects, compu...\n",
      "1    [dear, caps1, caps2, believe, using, computers...\n",
      "2    [dear, caps1, caps2, caps3, people, use, compu...\n",
      "3    [dear, local, newspaper, caps1, found, many, e...\n",
      "4    [dear, location1, know, computers, positive, e...\n",
      "Name: word_tokens, dtype: object\n",
      "0    [dear local newspaper, i think effects compute...\n",
      "1    [dear @caps1 @caps2, i believe that using comp...\n",
      "2    [dear, @caps1 @caps2 @caps3 more and more peop...\n",
      "3    [dear local newspaper, @caps1 i have found tha...\n",
      "4    [dear @location1, i know having computers has ...\n",
      "Name: sentence_tokens, dtype: object\n",
      "0    [dear, local, newspaper, think, effect, comput...\n",
      "1    [dear, caps1, caps2, believe, using, computer,...\n",
      "2    [dear, caps1, caps2, caps3, people, use, compu...\n",
      "3    [dear, local, newspaper, caps1, found, many, e...\n",
      "4    [dear, location1, know, computer, positive, ef...\n",
      "Name: lemmatized_word_tokens, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>ai_llm</th>\n",
       "      <th>ai_generated</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>lemmatized_word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, local, newspaper, think, effects, compu...</td>\n",
       "      <td>[dear local newspaper, i think effects compute...</td>\n",
       "      <td>[dear, local, newspaper, think, effect, comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, caps1, caps2, believe, using, computers...</td>\n",
       "      <td>[dear @caps1 @caps2, i believe that using comp...</td>\n",
       "      <td>[dear, caps1, caps2, believe, using, computer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, caps1, caps2, caps3, people, use, compu...</td>\n",
       "      <td>[dear, @caps1 @caps2 @caps3 more and more peop...</td>\n",
       "      <td>[dear, caps1, caps2, caps3, people, use, compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, local, newspaper, caps1, found, many, e...</td>\n",
       "      <td>[dear local newspaper, @caps1 i have found tha...</td>\n",
       "      <td>[dear, local, newspaper, caps1, found, many, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>human-generated</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, location1, know, computers, positive, e...</td>\n",
       "      <td>[dear @location1, i know having computers has ...</td>\n",
       "      <td>[dear, location1, know, computer, positive, ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24412</th>\n",
       "      <td>213</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nDear Editor, \\n\\nAs a citizen of this comm...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, editor, citizen, community, feel, impor...</td>\n",
       "      <td>[\\n\\ndear editor, \\n\\nas a citizen of this com...</td>\n",
       "      <td>[dear, editor, citizen, community, feel, impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24413</th>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\nDear Editor,\\n\\nAs a concerned citizen a...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, editor, concerned, citizen, longtime, r...</td>\n",
       "      <td>[\\n\\n\\ndear editor,\\n\\nas a concerned citizen ...</td>\n",
       "      <td>[dear, editor, concerned, citizen, longtime, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24414</th>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nDear Editor,\\n\\nI am writing to share my o...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, editor, writing, share, opinion, effect...</td>\n",
       "      <td>[\\n\\ndear editor,\\n\\ni am writing to share my ...</td>\n",
       "      <td>[dear, editor, writing, share, opinion, effect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24415</th>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nTo the Editor: \\n\\nAs our world has become...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>[editor, world, become, increasingly, reliant,...</td>\n",
       "      <td>[\\n\\nto the editor: \\n\\nas our world has becom...</td>\n",
       "      <td>[editor, world, become, increasingly, reliant,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24416</th>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nDear Editor, \\n\\nI am writing to present m...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, editor, writing, present, thoughts, eff...</td>\n",
       "      <td>[\\n\\ndear editor, \\n\\ni am writing to present ...</td>\n",
       "      <td>[dear, editor, writing, present, thought, effe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24417 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "24412       213          1  \\n\\nDear Editor, \\n\\nAs a citizen of this comm...   \n",
       "24413       214          1  \\n\\n\\nDear Editor,\\n\\nAs a concerned citizen a...   \n",
       "24414       215          1  \\n\\nDear Editor,\\n\\nI am writing to share my o...   \n",
       "24415       216          1  \\n\\nTo the Editor: \\n\\nAs our world has become...   \n",
       "24416       217          1  \\n\\nDear Editor, \\n\\nI am writing to present m...   \n",
       "\n",
       "                 ai_llm  ai_generated  \\\n",
       "0       human-generated             0   \n",
       "1       human-generated             0   \n",
       "2       human-generated             0   \n",
       "3       human-generated             0   \n",
       "4       human-generated             0   \n",
       "...                 ...           ...   \n",
       "24412  text-davinci-003             1   \n",
       "24413  text-davinci-003             1   \n",
       "24414  text-davinci-003             1   \n",
       "24415  text-davinci-003             1   \n",
       "24416  text-davinci-003             1   \n",
       "\n",
       "                                             word_tokens  \\\n",
       "0      [dear, local, newspaper, think, effects, compu...   \n",
       "1      [dear, caps1, caps2, believe, using, computers...   \n",
       "2      [dear, caps1, caps2, caps3, people, use, compu...   \n",
       "3      [dear, local, newspaper, caps1, found, many, e...   \n",
       "4      [dear, location1, know, computers, positive, e...   \n",
       "...                                                  ...   \n",
       "24412  [dear, editor, citizen, community, feel, impor...   \n",
       "24413  [dear, editor, concerned, citizen, longtime, r...   \n",
       "24414  [dear, editor, writing, share, opinion, effect...   \n",
       "24415  [editor, world, become, increasingly, reliant,...   \n",
       "24416  [dear, editor, writing, present, thoughts, eff...   \n",
       "\n",
       "                                         sentence_tokens  \\\n",
       "0      [dear local newspaper, i think effects compute...   \n",
       "1      [dear @caps1 @caps2, i believe that using comp...   \n",
       "2      [dear, @caps1 @caps2 @caps3 more and more peop...   \n",
       "3      [dear local newspaper, @caps1 i have found tha...   \n",
       "4      [dear @location1, i know having computers has ...   \n",
       "...                                                  ...   \n",
       "24412  [\\n\\ndear editor, \\n\\nas a citizen of this com...   \n",
       "24413  [\\n\\n\\ndear editor,\\n\\nas a concerned citizen ...   \n",
       "24414  [\\n\\ndear editor,\\n\\ni am writing to share my ...   \n",
       "24415  [\\n\\nto the editor: \\n\\nas our world has becom...   \n",
       "24416  [\\n\\ndear editor, \\n\\ni am writing to present ...   \n",
       "\n",
       "                                  lemmatized_word_tokens  \n",
       "0      [dear, local, newspaper, think, effect, comput...  \n",
       "1      [dear, caps1, caps2, believe, using, computer,...  \n",
       "2      [dear, caps1, caps2, caps3, people, use, compu...  \n",
       "3      [dear, local, newspaper, caps1, found, many, e...  \n",
       "4      [dear, location1, know, computer, positive, ef...  \n",
       "...                                                  ...  \n",
       "24412  [dear, editor, citizen, community, feel, impor...  \n",
       "24413  [dear, editor, concerned, citizen, longtime, r...  \n",
       "24414  [dear, editor, writing, share, opinion, effect...  \n",
       "24415  [editor, world, become, increasingly, reliant,...  \n",
       "24416  [dear, editor, writing, present, thought, effe...  \n",
       "\n",
       "[24417 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking that the previous code worked\n",
    "print(asap_df[\"word_tokens\"].head())\n",
    "print(asap_df[\"sentence_tokens\"].head())\n",
    "print(asap_df[\"lemmatized_word_tokens\"].head())\n",
    "asap_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted column\n",
    "asap_df = asap_df.drop(\"Unnamed: 0\", axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe\n",
    "asap_df.to_excel(\"../cleanData/3bProcessedAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       NaN\n",
      "1       NaN\n",
      "2       NaN\n",
      "3       NaN\n",
      "4       NaN\n",
      "         ..\n",
      "21445   NaN\n",
      "21446   NaN\n",
      "21447   NaN\n",
      "21448   NaN\n",
      "21449   NaN\n",
      "Name: word_tokens, Length: 21450, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Removing Special Charactes \n",
    "\n",
    "Remove_Special_CharactersDf=asap_df[\"essay\"].str.replace('\\W', ' ', regex=True)\n",
    "Remove_Special_CharactersDf\n",
    "sentence= Remove_Special_CharactersDf.loc[asap_df.index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/zz3zwwtj491ggdvwhnvk_wyr0000gn/T/ipykernel_21279/231292653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mRemove_Special_CharactersDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0masap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "# lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to\n",
    "# apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or \n",
    "# dictionary form of a word\n",
    "# Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships betweenits words\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "  \n",
    "sentence= Remove_Special_CharactersDf.loc[asap_df.index[1]]\n",
    " \n",
    "# tokenize the sentence and find the POS tag for each token\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) \n",
    "  \n",
    "# we use our own pos_tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    " \n",
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:       \n",
    "        # else use the tag to lemmatize the token\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    " \n",
    "print(lemmatized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear CAPS1 CAPS2 believe using computers benefit us many ways like talking becoming friends others websites like facebook mysace Using computers help us find coordibates locations able ourselfs millions information Also computers benefit us helping jobs planning house plan typing NUM1 page report one jobs less writing lets go wonder world technology Using computer help us life talking making friends line Many people myspace facebooks aim benefit us conversations one another Many people believe computers bad make friends never talk fortunate computer help school work social life make friends Computers help us finding locations coordibates millions information online go internet lot know go onto websites MONTH1 help us locations coordinates like LOCATION1 Would rather use computer LOCATION3 supposed vacationing LOCATION2 Million information found internet almost every question computer Would rather easily draw house plan computers take NUM1 hours one hand ugly erazer marks garrenteed find job drawing like Also appling job many workers must write long papers like NUM3 word essay job fits many people know like writing NUM3 words non stopp hours could take hav computer computers needed lot adays hope essay impacted descion computers great machines work day showed mom use computer said greatest invention sense sliced bread go buy computer help chat online friends find locations millions information one click button help self getting job neat prepared printed work boss love\n",
      "Old length:  2288\n",
      "New length:  1491\n"
     ]
    }
   ],
   "source": [
    "   #The words which are generally filtered out before processing a natural language are called stop words\n",
    "#Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\n",
    "#NLTK is a library to play with natural language.The steps to import the library and the English stop words list\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "\n",
    "words = [word for word in sentence.split() if word.lower() not in sw_nltk]\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)\n",
    "print(\"Old length: \", len(sentence))\n",
    "print(\"New length: \", len(new_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
