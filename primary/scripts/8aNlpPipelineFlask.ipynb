{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 18:53:54.254081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/cbarron/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.6.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "test_text = \"In the modern age, computers have revolutionized nearly every aspect of human life, significantly altering the way we work, communicate, and entertain ourselves. While their integration into society brings numerous advantages, it also presents certain challenges. This essay explores the pros and cons of computers in our society. On the positive side, computers have streamlined productivity and efficiency across various industries. They have simplified complex tasks, increasing accuracy and speed. Computers enable seamless communication, connecting people across the globe, fostering collaboration, and broadening cultural exchange. Moreover, they have revolutionized education, providing limitless resources and interactive learning experiences. Another advantage lies in the entertainment realm. Computers offer diverse multimedia experiences, from gaming to virtual reality, enhancing leisure options and relaxation. Furthermore, computers have facilitated medical advancements, from precise diagnoses to accelerated research. However, there are downsides to this pervasive technology. Overreliance on computers can lead to reduced physical activity and potential health issues. Social isolation is another concern as face-to-face interactions decrease in favor of virtual connections.\"\n",
    "\n",
    "#IMPORT DEPENDENCIES ################################################\n",
    "\n",
    "##lexical features modules ===========================================\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.corpus.stopwords import words\n",
    "\n",
    "wn = WordNetLemmatizer() #specifying wn as the word net lemmatizer\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "##syntactic features modules =========================================\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "#import openai\n",
    "# import pandas as pd\n",
    "# import nltk\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "## Stylistic features models =============\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "## passive sentence====\n",
    "import textstat\n",
    "\n",
    "# PREPROCESS TEXT #######################################\n",
    "\n",
    "#remove punctuation, remove stopwords\n",
    "def clean_text(text):\n",
    "    #sentences = nltk.sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in punctuation]) #remove punctuation\n",
    "    tokens = word_tokenize(text) #tokenize\n",
    "    text = [word for word in tokens if word not in stopwords_list] #remove stopwords\n",
    "    return(text)\n",
    "\n",
    "#create a function to lemmatize the text\n",
    "def lemmatize_text(word_tokens):\n",
    "    lem_text = [wn.lemmatize(word) for word in word_tokens]\n",
    "    return(lem_text)\n",
    "\n",
    "preproc_text = lemmatize_text(clean_text(test_text))\n",
    "\n",
    "\n",
    "# FEATURE EXTRACTION ####################################################\n",
    "from nltk import pos_tag\n",
    "\n",
    "## Function to extract lexical features ======================================\n",
    "def extract_lexical_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_word_count = len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    word_counts = Counter(words)\n",
    "    TTR = len(word_counts) / len(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    unique_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "    word_freq = word_counts\n",
    "    bigram_freq = Counter(ngrams(words, 2))\n",
    "    trigram_freq = Counter(ngrams(words, 3))\n",
    "    rare_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "\n",
    "    return {\n",
    "        'total_word_count': total_word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'TTR': TTR,\n",
    "        'stop_word_count': stop_word_count,\n",
    "        'unique_word_count': unique_word_count\n",
    "        #'word_freq': word_freq,\n",
    "        #'bigram_freq': bigram_freq,\n",
    "        #'trigram_freq': trigram_freq,\n",
    "        #'rare_word_count': rare_word_count\n",
    "    }\n",
    "\n",
    "all_lexical_features = extract_lexical_features(test_text)\n",
    "\n",
    "## Function to extract syntactic features =====================================\n",
    "def extract_syntactic_features(text):\n",
    "    # ... your extract_syntactic_features function implementation ...\n",
    "    doc = nlp_spacy(text)\n",
    "\n",
    "    # # Calculate average sentence length\n",
    "    # sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    # avg_sentence_length = np.mean(sentence_lengths)\n",
    "\n",
    "    # Calculate parse tree depth\n",
    "    def calc_tree_depth(sent):\n",
    "        root = [token for token in sent if token.head == token][0]\n",
    "        return max([len(list(token.ancestors)) for token in sent])\n",
    "\n",
    "    tree_depths = [calc_tree_depth(sent) for sent in doc.sents]\n",
    "    avg_parse_tree_depth = np.mean(tree_depths)\n",
    "    parse_tree_depth_variation = np.std(tree_depths)\n",
    "\n",
    "    return {\n",
    "        # 'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_parse_tree_depth': avg_parse_tree_depth,\n",
    "        'parse_tree_depth_variation': parse_tree_depth_variation,\n",
    "    }\n",
    "\n",
    "all_syntactic_features = extract_syntactic_features(test_text)\n",
    "\n",
    "##function to extract stylistic features ===============================================\n",
    "def extract_stylistic_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    pos_tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    \n",
    "    num_adjectives = sum(sum(1 for word, pos in sentence if pos.startswith('JJ')) for sentence in pos_tagged_sentences)\n",
    "    num_adverbs = sum(sum(1 for word, pos in sentence if pos.startswith('RB')) for sentence in pos_tagged_sentences)\n",
    "    num_verbs = sum(sum(1 for word, pos in sentence if pos.startswith('VB')) for sentence in pos_tagged_sentences)\n",
    "    num_nouns = sum(sum(1 for word, pos in sentence if pos.startswith('NN')) for sentence in pos_tagged_sentences)\n",
    "\n",
    "    avg_adjectives_per_sentence = num_adjectives / num_sentences\n",
    "    avg_adverbs_per_sentence = num_adverbs / num_sentences\n",
    "    avg_verbs_per_sentence = num_verbs / num_sentences\n",
    "    avg_nouns_per_sentence = num_nouns / num_sentences\n",
    "    \n",
    "    return {\n",
    "        'avg_adjectives_per_sentence': avg_adjectives_per_sentence,\n",
    "        'avg_adverbs_per_sentence': avg_adverbs_per_sentence,\n",
    "        'avg_verbs_per_sentence': avg_verbs_per_sentence,\n",
    "        'avg_nouns_per_sentence': avg_nouns_per_sentence,\n",
    "    }\n",
    "\n",
    "all_stylistic_features = extract_stylistic_features(test_text)\n",
    "\n",
    "##function to calculate punctuation ========================\n",
    "def count_punctuation(text):\n",
    "    punctuation_count = sum(1 for char in text if char in punctuation)\n",
    "    punct_length = sum(1 for char in text)\n",
    "    punctuation_proportion = punctuation_count / punct_length\n",
    "    return {\"punctuation_proportion\" :punctuation_proportion}\n",
    "\n",
    "all_avg_punctuation = count_punctuation(test_text)\n",
    "\n",
    "## Function to count passive sentences =========================\n",
    "def count_passive_sentences(text):\n",
    "    passive_sentences = 0\n",
    "    doc = nlp_spacy(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            passive_sentences += 1\n",
    "    return {\"passive_sentences\" : passive_sentences}\n",
    "\n",
    "passive_sentence_feature = count_passive_sentences(test_text)\n",
    "\n",
    "\n",
    "## Function to calculate readability scores ==========================\n",
    "def readability_scores(text):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.text_standard(text, float_output=True)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    return {\n",
    "        \"flesch_reading_ease\" : flesch_reading_ease, \n",
    "        \"flesch_kincaid_grade_level\" : flesch_kincaid_grade_level, \n",
    "        \"smog_index\" : smog_index}\n",
    "\n",
    "readability_feature = readability_scores(test_text)\n",
    "\n",
    "## Function to calculate sentiment analysis scores ====================\n",
    "def sentiment_analysis_scores(text):\n",
    "    sentiment = TextBlob(text)\n",
    "    return {\n",
    "        \"sentiment_polarity\" : sentiment.polarity, \n",
    "        \"sentiment_subjectivity\" : sentiment.subjectivity}\n",
    "\n",
    "sentiment_feature = sentiment_analysis_scores(test_text)\n",
    "\n",
    "# COMBINE ALL FEATURES\n",
    "features = {**all_lexical_features, **all_syntactic_features, **all_stylistic_features, **all_avg_punctuation, **passive_sentence_feature, **readability_feature, **sentiment_feature}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_word_count': 196,\n",
       " 'avg_word_length': 5.755102040816326,\n",
       " 'avg_sentence_length': 12.76923076923077,\n",
       " 'TTR': 0.6836734693877551,\n",
       " 'stop_word_count': 50,\n",
       " 'unique_word_count': 118,\n",
       " 'avg_parse_tree_depth': 3.923076923076923,\n",
       " 'parse_tree_depth_variation': 0.9166442529086912,\n",
       " 'avg_adjectives_per_sentence': 1.5384615384615385,\n",
       " 'avg_adverbs_per_sentence': 0.46153846153846156,\n",
       " 'avg_verbs_per_sentence': 2.3076923076923075,\n",
       " 'avg_nouns_per_sentence': 4.846153846153846,\n",
       " 'punctuation_proportion': 0.02474864655839134,\n",
       " 'passive_sentences': 0,\n",
       " 'flesch_reading_ease': 7.72,\n",
       " 'flesch_kincaid_grade_level': 16.0,\n",
       " 'smog_index': 15.8,\n",
       " 'sentiment_polarity': 0.09061823593073594,\n",
       " 'sentiment_subjectivity': 0.4000879329004328}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'modern',\n",
       " 'age',\n",
       " 'computers',\n",
       " 'have',\n",
       " 'revolutionized',\n",
       " 'nearly',\n",
       " 'every',\n",
       " 'aspect',\n",
       " 'of',\n",
       " 'human',\n",
       " 'life',\n",
       " 'significantly',\n",
       " 'altering',\n",
       " 'the',\n",
       " 'way',\n",
       " 'we',\n",
       " 'work',\n",
       " 'communicate',\n",
       " 'and',\n",
       " 'entertain',\n",
       " 'ourselves',\n",
       " 'while',\n",
       " 'their',\n",
       " 'integration',\n",
       " 'into',\n",
       " 'society',\n",
       " 'brings',\n",
       " 'numerous',\n",
       " 'advantages',\n",
       " 'it',\n",
       " 'also',\n",
       " 'presents',\n",
       " 'certain',\n",
       " 'challenges',\n",
       " 'this',\n",
       " 'essay',\n",
       " 'explores',\n",
       " 'the',\n",
       " 'pros',\n",
       " 'and',\n",
       " 'cons',\n",
       " 'of',\n",
       " 'computers',\n",
       " 'in',\n",
       " 'our',\n",
       " 'society',\n",
       " 'on',\n",
       " 'the',\n",
       " 'positive',\n",
       " 'side',\n",
       " 'computers',\n",
       " 'have',\n",
       " 'streamlined',\n",
       " 'productivity',\n",
       " 'and',\n",
       " 'efficiency',\n",
       " 'across',\n",
       " 'various',\n",
       " 'industries',\n",
       " 'they',\n",
       " 'have',\n",
       " 'simplified',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'increasing',\n",
       " 'accuracy',\n",
       " 'and',\n",
       " 'speed',\n",
       " 'computers',\n",
       " 'enable',\n",
       " 'seamless',\n",
       " 'communication',\n",
       " 'connecting',\n",
       " 'people',\n",
       " 'across',\n",
       " 'the',\n",
       " 'globe',\n",
       " 'fostering',\n",
       " 'collaboration',\n",
       " 'and',\n",
       " 'broadening',\n",
       " 'cultural',\n",
       " 'exchange',\n",
       " 'moreover',\n",
       " 'they',\n",
       " 'have',\n",
       " 'revolutionized',\n",
       " 'education',\n",
       " 'providing',\n",
       " 'limitless',\n",
       " 'resources',\n",
       " 'and',\n",
       " 'interactive',\n",
       " 'learning',\n",
       " 'experiences',\n",
       " 'another',\n",
       " 'advantage',\n",
       " 'lies',\n",
       " 'in',\n",
       " 'the',\n",
       " 'entertainment',\n",
       " 'realm',\n",
       " 'computers',\n",
       " 'offer',\n",
       " 'diverse',\n",
       " 'multimedia',\n",
       " 'experiences',\n",
       " 'from',\n",
       " 'gaming',\n",
       " 'to',\n",
       " 'virtual',\n",
       " 'reality',\n",
       " 'enhancing',\n",
       " 'leisure',\n",
       " 'options',\n",
       " 'and',\n",
       " 'relaxation',\n",
       " 'furthermore',\n",
       " 'computers',\n",
       " 'have',\n",
       " 'facilitated',\n",
       " 'medical',\n",
       " 'advancements',\n",
       " 'from',\n",
       " 'precise',\n",
       " 'diagnoses',\n",
       " 'to',\n",
       " 'accelerated',\n",
       " 'research',\n",
       " 'however',\n",
       " 'there',\n",
       " 'are',\n",
       " 'downsides',\n",
       " 'to',\n",
       " 'this',\n",
       " 'pervasive',\n",
       " 'technology',\n",
       " 'overreliance',\n",
       " 'on',\n",
       " 'computers',\n",
       " 'can',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'reduced',\n",
       " 'physical',\n",
       " 'activity',\n",
       " 'and',\n",
       " 'potential',\n",
       " 'health',\n",
       " 'issues',\n",
       " 'social',\n",
       " 'isolation',\n",
       " 'is',\n",
       " 'another',\n",
       " 'concern',\n",
       " 'as',\n",
       " 'facetoface',\n",
       " 'interactions',\n",
       " 'decrease',\n",
       " 'in',\n",
       " 'favor',\n",
       " 'of',\n",
       " 'virtual',\n",
       " 'connections']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Essay perplexity =================\n",
    "\n",
    "# #install dependencies\n",
    "# #install dependencies\n",
    "# from nltk.lm.preprocessing import padded_everygram_pipeline #function used to prepare the tokenized text accordingly\n",
    "# from nltk.lm import MLE\n",
    "# from nltk.util import bigrams\n",
    "# from nltk.lm.preprocessing import pad_both_ends\n",
    "# from nltk.lm import MLE #import a maximum likelihood estimator\n",
    "\n",
    "def clean_text_keep_stopword(text):\n",
    "    sentences = sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in punctuation]) #remove punctuation\n",
    "    tokens = word_tokenize(text) #tokenize\n",
    "    return(tokens)\n",
    "\n",
    "word_token_with_stopword = clean_text_keep_stopword(test_text.lower())\n",
    "\n",
    "#use padded_everygram_pipeline() to preprocess the tokenized data \n",
    "train, vocab = padded_everygram_pipeline(2, word_token_with_stopword)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate essay perplexity\n",
    "\n",
    "#install dependencies\n",
    "#install dependencies\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline #function used to prepare the tokenized text accordingly\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import string\n",
    "\n",
    "#install the “popular” subset of NLTK data, on the command line type\n",
    "#python -m nltk.downloader popular\n",
    "from nltk import tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.lm import MLE #import a maximum likelihood estimator\n",
    "\n",
    "#function to tokenize text while keeping stop words\n",
    "def clean_text_keep_stopword(text):\n",
    "    sentences = sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    tokens = word_tokenize(text) #tokenize\n",
    "    return(tokens)\n",
    "\n",
    "#preprocessing for perplexity\n",
    "perplexty_df = merged_df.loc[:, [\"essay_id\", \"ai_llm\", \"essay\"]]\n",
    "\n",
    "#run the word tokenized function through every row of text\n",
    "perplexty_df['word_token_with_stopword'] = perplexty_df[\"essay\"].apply(lambda x: clean_text_keep_stopword(x.lower())) \n",
    "\n",
    "#use padded_everygram_pipeline() to preprocess the tokenized data \n",
    "train, vocab = padded_everygram_pipeline(2, perplexty_df['word_token_with_stopword'])\n",
    "\n",
    "#import a maximum likelihood estimator\n",
    "from nltk.lm import MLE \n",
    "lm = MLE(2) #use MLE function to create an empty vocabulary\n",
    "\n",
    " #fit the MLE model to the preprocessed data\n",
    "lm.fit(train, vocab)\n",
    "\n",
    "#use a list comprehension to run two functions on the word tokens with stopwords for each essay:\n",
    " #1) create a series of bigrams based on the text, 2) calculate entropy\n",
    "merged_df[\"perplexity\"] = perplexty_df['word_token_with_stopword'].apply(lambda x: lm.perplexity(list(bigrams(pad_both_ends(x, n=2)))))\n",
    "\n",
    "#save the dataset\n",
    "merged_df.to_excel(\"../cleanData/4aFeaturesAsap.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE CODE FROM \n",
    " #https://www.freecodecamp.org/news/how-to-deploy-an-nlp-model-with-fastapi/\n",
    "# cleaning the data\n",
    "# text preprocessing modules\n",
    "from string import punctuation\n",
    "# text preprocessing modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re  # regular expression\n",
    "import os\n",
    "from os.path import dirname, join, realpath\n",
    "import joblib\n",
    "import uvicorn\n",
    "from fastapi import FastAPI \n",
    "\n",
    "def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):\n",
    "    # Clean the text, with the option to remove stop_words and to lemmatize word\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+\", \" link \", text)\n",
    "    text = re.sub(r\"\\b\\d+(?:\\.\\d+)?\\s+\", \"\", text)  # remove numbers\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = \"\".join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        # load stopwords\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    # Optionally, shorten words to their stems\n",
    "    if lemmatize_words:\n",
    "        text = text.split()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "        text = \" \".join(lemmatized_words)\n",
    "        \n",
    "    # Return a list of words\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to save the perplexity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate essay perplexity\n",
    "\n",
    "#install dependencies\n",
    "#install dependencies\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline #function used to prepare the tokenized text accordingly\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import string\n",
    "\n",
    "#install the “popular” subset of NLTK data, on the command line type\n",
    "#python -m nltk.downloader popular\n",
    "import nltk.tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.lm import MLE #import a maximum likelihood estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tokenize text while keeping stop words\n",
    "def clean_text_keep_stopword(text):\n",
    "    sentences = nltk.sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    tokens = nltk.tokenize.word_tokenize(text) #tokenize\n",
    "    return(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_excel('../cleanData/3bProcessedAsap.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing for perplexity\n",
    "perplexty_df = merged_df.loc[:, [\"essay_id\", \"ai_llm\", \"essay\"]]\n",
    "\n",
    "#run the word tokenized function through every row of text\n",
    "perplexty_df['word_token_with_stopword'] = perplexty_df[\"essay\"].apply(lambda x: clean_text_keep_stopword(x.lower())) \n",
    "\n",
    "#use padded_everygram_pipeline() to preprocess the tokenized data \n",
    "train, vocab = padded_everygram_pipeline(2, perplexty_df['word_token_with_stopword'])\n",
    "\n",
    "#import a maximum likelihood estimator\n",
    "from nltk.lm import MLE \n",
    "lm = MLE(2) #use MLE function to create an empty vocabulary\n",
    "\n",
    " #fit the MLE model to the preprocessed data\n",
    "lm.fit(train, vocab)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a list comprehension to run two functions on the word tokens with stopwords for each essay:\n",
    " #1) create a series of bigrams based on the text, 2) calculate entropy\n",
    "merged_df[\"perplexity\"] = perplexty_df['word_token_with_stopword'].apply(lambda x: lm.perplexity(list(bigrams(pad_both_ends(x, n=2)))))\n",
    "\n",
    "#save the dataset\n",
    "merged_df.to_excel(\"../cleanData/4aFeaturesAsap.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
