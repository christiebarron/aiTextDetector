{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab5317e0",
   "metadata": {},
   "source": [
    "# NLP Pipeline Jupyter Notebook for the aiTextDetect Project: Step 3\n",
    "\n",
    "## Extracting Features from Preprocessed data\n",
    "\n",
    "This script extracts a series of features from the text, saving them in a pandas dataframe that is then saved as `cleanData/4afeaturesAsap.xlsx`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c6a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import openai\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc5dd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract lexical features\n",
    "def extract_lexical_features(text):\n",
    "    # ... your extract_lexical_features function implementation ...\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    total_word_count = len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    word_counts = Counter(words)\n",
    "    TTR = len(word_counts) / len(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    unique_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "    word_freq = word_counts\n",
    "    bigram_freq = Counter(ngrams(words, 2))\n",
    "    trigram_freq = Counter(ngrams(words, 3))\n",
    "    rare_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "\n",
    "    return {\n",
    "        'total_word_count': total_word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'TTR': TTR,\n",
    "        'stop_word_count': stop_word_count,\n",
    "        'unique_word_count': unique_word_count,\n",
    "        'word_freq': word_freq,\n",
    "        'bigram_freq': bigram_freq,\n",
    "        'trigram_freq': trigram_freq,\n",
    "        'rare_word_count': rare_word_count\n",
    "    }\n",
    "\n",
    "#load data from excel file and save as list\n",
    "merged_df = pd.read_excel('../cleanData/3bProcessedAsap.xlsx')\n",
    "all_essays = merged_df['essay'].tolist()\n",
    "\n",
    "# Extract lexical features from AI-generated and human-written essays\n",
    "all_lexical_features = [extract_lexical_features(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c23d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_lexical_features)], axis = 1)#.to_excel(\"../cleanData/featuresAsap.xlsx\")\n",
    "merged_df.to_excel(\"../cleanData/4afeaturesAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6b99076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract syntactic features\n",
    "def extract_syntactic_features(text):\n",
    "    # ... your extract_syntactic_features function implementation ...\n",
    "    doc = nlp_spacy(text)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    avg_sentence_length = np.mean(sentence_lengths)\n",
    "\n",
    "    # Calculate parse tree depth\n",
    "    def calc_tree_depth(sent):\n",
    "        root = [token for token in sent if token.head == token][0]\n",
    "        return max([len(list(token.ancestors)) for token in sent])\n",
    "\n",
    "    tree_depths = [calc_tree_depth(sent) for sent in doc.sents]\n",
    "    avg_parse_tree_depth = np.mean(tree_depths)\n",
    "    parse_tree_depth_variation = np.std(tree_depths)\n",
    "\n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_parse_tree_depth': avg_parse_tree_depth,\n",
    "        'parse_tree_depth_variation': parse_tree_depth_variation,\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract syntactic features from AI-generated and human-written essays\n",
    "all_syntactic_features = [extract_syntactic_features(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "873a75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_syntactic_features)], axis = 1)#.to_excel(\"../cleanData/features.Asap.xlsx\")\n",
    "merged_df.to_excel(\"../cleanData/4afeaturesAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4793bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine lexical and syntactic features\n",
    "# def combined_features(text):\n",
    "#     lexical = extract_lexical_features(text)\n",
    "#     syntactic = extract_syntactic_features(text)\n",
    "#     return {**lexical, **syntactic}\n",
    "\n",
    "# # Extract combined features for AI-generated and human-written essays\n",
    "# all_combined_features = [combined_features(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5864d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stylistic Features\n",
    "def extract_stylistic_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    pos_tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    \n",
    "    num_adjectives = sum(sum(1 for word, pos in sentence if pos.startswith('JJ')) for sentence in pos_tagged_sentences)\n",
    "    num_adverbs = sum(sum(1 for word, pos in sentence if pos.startswith('RB')) for sentence in pos_tagged_sentences)\n",
    "    num_verbs = sum(sum(1 for word, pos in sentence if pos.startswith('VB')) for sentence in pos_tagged_sentences)\n",
    "    num_nouns = sum(sum(1 for word, pos in sentence if pos.startswith('NN')) for sentence in pos_tagged_sentences)\n",
    "\n",
    "    avg_adjectives_per_sentence = num_adjectives / num_sentences\n",
    "    avg_adverbs_per_sentence = num_adverbs / num_sentences\n",
    "    avg_verbs_per_sentence = num_verbs / num_sentences\n",
    "    avg_nouns_per_sentence = num_nouns / num_sentences\n",
    "    \n",
    "    return {\n",
    "        'avg_adjectives_per_sentence': avg_adjectives_per_sentence,\n",
    "        'avg_adverbs_per_sentence': avg_adverbs_per_sentence,\n",
    "        'avg_verbs_per_sentence': avg_verbs_per_sentence,\n",
    "        'avg_nouns_per_sentence': avg_nouns_per_sentence,\n",
    "    }\n",
    "\n",
    "# Extract stylistic features from AI-generated and human-written essays\n",
    "all_stylistic_features = [extract_stylistic_features(essay) for essay in all_essays]\n",
    "\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
    "    punct_length = sum(1 for char in text)\n",
    "    punctuation_proportion = punctuation_count / punct_length\n",
    "    return {\"punctuation_proportion\" :punctuation_proportion}\n",
    "\n",
    "all_avg_punctuation = [count_punctuation(essay) for essay in all_essays]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d55d995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_stylistic_features)], axis = 1)#   pd.DataFrame(all_avg_punctuation)\n",
    "merged_df.to_excel(\"../cleanData/4afeaturesAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a47ce524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/cbarron/opt/anaconda3/envs/env39/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /Users/cbarron/opt/anaconda3/envs/env39/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/cbarron/opt/anaconda3/envs/env39/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/cbarron/opt/anaconda3/envs/env39/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/cbarron/opt/anaconda3/envs/env39/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26ce831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Using cached textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Using cached pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.14.0 textstat-0.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4ce3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to count passive sentences\n",
    "def count_passive_sentences(text):\n",
    "    passive_sentences = 0\n",
    "    doc = nlp_spacy(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            passive_sentences += 1\n",
    "    return passive_sentences\n",
    "\n",
    "# Function to calculate readability scores\n",
    "#from readability import Readability\n",
    "#from readability.exceptions import ReadabilityException\n",
    "\n",
    "\n",
    "import textstat\n",
    "\n",
    "# Function to calculate readability scores\n",
    "def readability_scores(text):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.text_standard(text, float_output=True)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    return {\n",
    "        \"flesch_reading_ease\" : flesch_reading_ease, \n",
    "        \"flesch_kincaid_grade_level\" : flesch_kincaid_grade_level, \n",
    "        \"smog_index\" : smog_index}\n",
    "\n",
    "\n",
    "# Function to calculate sentiment analysis scores\n",
    "def sentiment_analysis_scores(text):\n",
    "    sentiment = TextBlob(text)\n",
    "    return {\n",
    "        \"sentiment_polarity\" : sentiment.polarity, \n",
    "        \"sentiment.subjectivity\" : sentiment.subjectivity}\n",
    "\n",
    "# Calculate new features for AI-generated and human-written essays\n",
    "all_passive_sentences = [count_passive_sentences(essay) for essay in all_essays]\n",
    "    #ai_generated_passive_sentences = [count_passive_sentences(essay) for essay in ai_generated_essays]\n",
    "    #human_written_passive_sentences = [count_passive_sentences(essay) for essay in human_written_essays]\n",
    "\n",
    "\n",
    "all_readibility_scores = [readability_scores(essay) for essay in all_essays]\n",
    "    #ai_generated_readability_scores = [readability_scores(essay) for essay in ai_generated_essays]\n",
    "    #human_written_readability_scores = [readability_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "all_sentiment_scores = [sentiment_analysis_scores(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "725723a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the extracted features with the original data\n",
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_passive_sentences), pd.DataFrame(all_readibility_scores), pd.DataFrame(all_sentiment_scores)], axis = 1) #pd.DataFrame(all_lexical_features), pd.DataFrame(all_syntactic_features), pd.DataFrame(all_stylistic_features)\n",
    "\n",
    "#save as excel document\n",
    "merged_df.to_excel(\"../cleanData/4afeaturesAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b770cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate essay perplexity\n",
    "\n",
    "#install dependencies\n",
    "#install dependencies\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline #function used to prepare the tokenized text accordingly\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import string\n",
    "\n",
    "#install the “popular” subset of NLTK data, on the command line type\n",
    "#python -m nltk.downloader popular\n",
    "import nltk.tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.lm import MLE #import a maximum likelihood estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f68b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tokenize text while keeping stop words\n",
    "def clean_text_keep_stopword(text):\n",
    "    sentences = nltk.sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    tokens = nltk.tokenize.word_tokenize(text) #tokenize\n",
    "    return(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d0a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing for perplexity\n",
    "perplexty_df = merged_df.loc[:, [\"essay_id\", \"ai_llm\", \"essay\"]]\n",
    "\n",
    "#run the word tokenized function through every row of text\n",
    "perplexty_df['word_token_with_stopword'] = perplexty_df[\"essay\"].apply(lambda x: clean_text_keep_stopword(x.lower())) \n",
    "\n",
    "#use padded_everygram_pipeline() to preprocess the tokenized data \n",
    "train, vocab = padded_everygram_pipeline(2, perplexty_df['word_token_with_stopword'])\n",
    "\n",
    "#import a maximum likelihood estimator\n",
    "from nltk.lm import MLE \n",
    "lm = MLE(2) #use MLE function to create an empty vocabulary\n",
    "\n",
    " #fit the MLE model to the preprocessed data\n",
    "lm.fit(train, vocab)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a list comprehension to run two functions on the word tokens with stopwords for each essay:\n",
    " #1) create a series of bigrams based on the text, 2) calculate entropy\n",
    "merged_df[\"perplexity\"] = perplexty_df['word_token_with_stopword'].apply(lambda x: lm.perplexity(list(bigrams(pad_both_ends(x, n=2)))))\n",
    "\n",
    "#save the dataset\n",
    "merged_df.to_excel(\"../cleanData/4aFeaturesAsap.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "253f93bd",
   "metadata": {},
   "source": [
    "## Calculating Averages Based on Features\n",
    "\n",
    "This remaining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414792e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values of some lexical features for both AI-generated and human-written essays\n",
    "def average_feature_value(features, feature_key):\n",
    "    return sum(feature[feature_key] for feature in features) / len(features)\n",
    "\n",
    "ai_avg_word_length = average_feature_value(ai_generated_features, 'avg_word_length')\n",
    "ai_avg_TTR = average_feature_value(ai_generated_features, 'TTR')\n",
    "ai_avg_stop_word_count = average_feature_value(ai_generated_features, 'stop_word_count')\n",
    "\n",
    "ai_avg_sentence_length = average_feature_value(ai_generated_features, 'avg_sentence_length')\n",
    "human_avg_sentence_length = average_feature_value(human_written_features, 'avg_sentence_length')\n",
    "human_avg_word_length = average_feature_value(human_written_features, 'avg_word_length')\n",
    "human_avg_TTR = average_feature_value(human_written_features, 'TTR')\n",
    "human_avg_stop_word_count = average_feature_value(human_written_features, 'stop_word_count')\n",
    "\n",
    "# Calculate average values of the total word count for both AI-generated and human-written essays\n",
    "ai_avg_total_word_count = average_feature_value(ai_generated_features, 'total_word_count')\n",
    "human_avg_total_word_count = average_feature_value(human_written_features, 'total_word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the syntactic features\n",
    "ai_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in ai_generated_syntactic_features])\n",
    "human_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in ai_generated_syntactic_features])\n",
    "human_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in ai_generated_syntactic_features])\n",
    "human_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in human_written_syntactic_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values of stylistic features for both AI-generated and human-written essays\n",
    "ai_avg_adjectives_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "ai_avg_adverbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "ai_avg_verbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_verbs_per_sentence')\n",
    "ai_avg_nouns_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "human_avg_adjectives_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "human_avg_adverbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "human_avg_verbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_verbs_per_sentence')\n",
    "human_avg_nouns_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
    "    return punctuation_count\n",
    "\n",
    "def average_value(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "all_avg_punctuation = average_value([count_punctuation(essay) for essay in all_essays])\n",
    "ai_avg_punctuation = average_value([count_punctuation(essay) for essay in ai_generated_essays])\n",
    "human_avg_punctuation = average_value([count_punctuation(essay) for essay in human_written_essays])\n",
    "\n",
    "\n",
    "ai_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in ai_generated_essays])\n",
    "human_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in human_written_essays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', \n",
    "                'Average Word Length', \n",
    "                'Average Sentence Length', \n",
    "                'Type-Token Ratio', \n",
    "                'Stop Word Count', \n",
    "                'Average Parse Tree Depth', \n",
    "                'Parse Tree Depth Variation', \n",
    "                'Average Adjectives per Sentence', \n",
    "                'Average Adverbs per Sentence', \n",
    "                'Average Verbs per Sentence', \n",
    "                'Average Nouns per Sentence', \n",
    "                'Average Punctuation Marks'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, \n",
    "                     ai_avg_word_length, ai_avg_sentence_length, \n",
    "                     ai_avg_TTR, ai_avg_stop_word_count, \n",
    "                     ai_avg_parse_tree_depth, \n",
    "                     ai_parse_tree_depth_variation, \n",
    "                     ai_avg_adjectives_per_sentence, \n",
    "                     ai_avg_adverbs_per_sentence, \n",
    "                     ai_avg_verbs_per_sentence, \n",
    "                     ai_avg_nouns_per_sentence, \n",
    "                     ai_avg_punctuation],\n",
    "    'Human-Written': [human_avg_total_word_count, \n",
    "                      human_avg_word_length, \n",
    "                      human_avg_sentence_length, \n",
    "                      human_avg_TTR, \n",
    "                      human_avg_stop_word_count, \n",
    "                      human_avg_parse_tree_depth, \n",
    "                      human_parse_tree_depth_variation, \n",
    "                      human_avg_adjectives_per_sentence, \n",
    "                      human_avg_adverbs_per_sentence, \n",
    "                      human_avg_verbs_per_sentence, \n",
    "                      human_avg_nouns_per_sentence, \n",
    "                      human_avg_punctuation],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# # Save the updated comparison DataFrame to an Excel file\n",
    "# comparison_df.to_excel('feature_comparison.xlsx', index=False)\n",
    "import openpyxl\n",
    "# Save the comparison DataFrame to an Excel file\n",
    "file_name = 'feature_comparison.xlsx'\n",
    "comparison_df.to_excel(file_name, index=False)\n",
    "\n",
    "# Autofit the column widths using openpyxl\n",
    "workbook = openpyxl.load_workbook(file_name)\n",
    "worksheet = workbook.active\n",
    "\n",
    "for column_cells in worksheet.columns:\n",
    "    length = max(len(str(cell.value)) for cell in column_cells)\n",
    "    worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "workbook.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values for the new features\n",
    "ai_avg_passive_sentences = np.mean(ai_generated_passive_sentences)\n",
    "human_avg_passive_sentences = np.mean(human_written_passive_sentences)\n",
    "\n",
    "ai_avg_flesch_reading_ease = np.mean([score[0] for score in ai_generated_readability_scores])\n",
    "human_avg_flesch_reading_ease = np.mean([score[0] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_smog_index = np.mean([score[1] for score in ai_generated_readability_scores])\n",
    "human_avg_smog_index = np.mean([score[1] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_polarity = np.mean([score[0] for score in ai_generated_sentiment_scores])\n",
    "human_avg_polarity = np.mean([score[0] for score in human_written_sentiment_scores])\n",
    "\n",
    "ai_avg_subjectivity = np.mean([score[1] for score in ai_generated_sentiment_scores])\n",
    "human_avg_subjectivity = np.mean([score[1] for score in human_written_sentiment_scores])\n",
    "\n",
    "# Update comparison_data with the new features\n",
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', 'Average Word Length', 'Average Sentence Length', 'Type-Token Ratio', 'Stop Word Count', 'Avg Parse Tree Depth', 'Parse Tree Depth Variation', 'Punctuation Count', 'Passive Sentences', 'Flesch Reading Ease', 'SMOG Index', 'Sentiment Polarity', 'Sentiment Subjectivity'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, ai_avg_word_length, ai_avg_sentence_length, ai_avg_TTR, ai_avg_stop_word_count, ai_avg_parse_tree_depth, ai_parse_tree_depth_variation, ai_avg_punctuation, ai_avg_passive_sentences, ai_avg_flesch_reading_ease, ai_avg_smog_index, ai_avg_polarity, ai_avg_subjectivity],\n",
    "    'Human-Written': [human_avg_total_word_count, human_avg_word_length, human_avg_sentence_length, human_avg_TTR, human_avg_stop_word_count, human_avg_parse_tree_depth, human_parse_tree_depth_variation, human_avg_punctuation, human_avg_passive_sentences, human_avg_flesch_reading_ease, human_avg_smog_index, human_avg_polarity, human_avg_subjectivity],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
